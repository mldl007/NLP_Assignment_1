{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b74417-6f61-4ab1-861b-f6d8d351e1e5",
   "metadata": {},
   "source": [
    "<h4>1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8df60-1a9a-4243-85c6-1aa99f08b101",
   "metadata": {},
   "source": [
    "One-hot encoding converts a categorical variable into numerical form by assigning 1 where a category occurs and 0s to the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8352e-1e7d-47ec-8ea0-5fa895270666",
   "metadata": {},
   "source": [
    "<h4>2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0c8a4-accc-45d3-9df8-85900ac625e1",
   "metadata": {},
   "source": [
    "Bag of words is a text vectorization technique where the tokens/words in a corpus are the columns. Each document is represented as a row. Cell (intersection of row and columns) where the token is present in a document is assigned with the token frequency in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc56aad-a606-4938-8977-53dbcd119054",
   "metadata": {},
   "source": [
    "<h4>3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398436d2-54a0-4dcb-8415-94572fbf3c3e",
   "metadata": {},
   "source": [
    "Bag of n-grams is similar to bag of words, but the columns represent n-grams rather than tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06cb4d-9f4f-46f8-b0c2-eafea6f189b1",
   "metadata": {},
   "source": [
    "<h4>4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1dc444-17c1-4d39-a052-3c8d93692fee",
   "metadata": {},
   "source": [
    "TF-iDF is a text vectorization technique where a word occuring with high frequency in all the documents is given low weight (stop words). But token occuring with high frequency in a document but lower frequency accross documents is given a higher weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0437b-f316-41dd-a626-2a1353c192e7",
   "metadata": {},
   "source": [
    "<h4>5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf47213-3030-493a-9d4f-a86da60790e2",
   "metadata": {},
   "source": [
    "Out of vocabulary probelm occurs when a token during prediction is not found while training or in the training vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90207f83-4d7c-4dc8-abb2-6d0b5628142f",
   "metadata": {},
   "source": [
    "<h4>6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31554fd-675f-4bbc-9ced-e009f17eed0f",
   "metadata": {},
   "source": [
    "Word embeddings are vectorized representation of tokens in a document. Word embeddings are learned by training a single layered neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f88f5-33ad-4eab-b748-ceb7dacb9ec6",
   "metadata": {},
   "source": [
    "<h4>7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc38060-9a2d-42eb-9aa9-8c331d982e21",
   "metadata": {},
   "source": [
    "CBOW is a word embedding training technique where a target token is predicted using the surrounding tokens in a window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c41918-dc25-446d-92a7-c6cd987ba4c6",
   "metadata": {},
   "source": [
    "<h4>8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51508f-a56a-493d-b62c-61274a5cb5df",
   "metadata": {},
   "source": [
    "Skip gram is a word embedding training technique where the surrounding words in a window are predicted using the center word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b86623-f940-4557-8665-8e85f1901950",
   "metadata": {},
   "source": [
    "<h4>9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71f7db-f216-48c9-b394-f23d7b299612",
   "metadata": {},
   "source": [
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
